{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path, walk, makedirs\n",
    "from collections import defaultdict\n",
    "from re import compile, sub\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Конвертация .pdf в сырой текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = ''\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Конвертация данных конференций из .pdf в текстовые файлы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conferences = ['Dialogue', 'AIST', 'AINL', 'RuSSIR']\n",
    "ds_store = '.DS_Store'\n",
    "saving_dir = 'prepared-data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert():\n",
    "    papers = []\n",
    "    for conference in conferences:\n",
    "        for year in range(2000, 2018):\n",
    "            for root, dirs, files in walk(path.join('..', 'data', conference, str(year)), 'rb'):\n",
    "                    for file in files:\n",
    "                        if (str(file)) != ds_store:\n",
    "                            try:\n",
    "                                papers.append(convert_pdf_to_txt('{}/{}'.format(root, file)))  \n",
    "                            except:\n",
    "                                continue\n",
    "            for paper_id, paper in enumerate(papers):\n",
    "                directory = path.join('..', saving_dir, conference, str(year))\n",
    "                if not path.exists(directory):\n",
    "                    makedirs(directory)\n",
    "                with open(path.join(directory, str(paper_id)) + '.txt', 'w') as f:\n",
    "                    f.write(paper)      \n",
    "            papers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парсинг полученных текстовых файлов\n",
    "\n",
    "Подготовка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_bad(string):\n",
    "    return sub('\\t|\\xa0', '', string.replace('\\n', ' ')).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encounter_utility_file(filename):\n",
    "    if any(name in filename for name in ['DS_Store', 'zip']):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TITLE = 'title'\n",
    "AUTHOR = 'author'\n",
    "NAME = 'name'\n",
    "AFFILIATION = 'affiliation'\n",
    "EMAIL = 'email'\n",
    "TEXT = 'text'\n",
    "REFERENCES = 'references'\n",
    "REF_MARKER = '\\xa0 ЛИТЕРАТУРА \\xa0'\n",
    "KEYWORDS_MARKER = 'Ключевые слова'\n",
    "KEYWORDS = 'keywords'\n",
    "ABSTRACT = 'abstract'\n",
    "CONFERENCE = 'conference'\n",
    "YEAR = 'year'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парсим Диалог\n",
    "\n",
    "Из-за разных форматов статей за разные годы конференций я сделал две отдельные функции для парсинга для разных наборов годов (200-2007 и 2008-2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_paper_data(content, conference, year):\n",
    "    if int(year) > 2007:\n",
    "        parse_dialogue_2007_plus(content, conference, year)\n",
    "    else:\n",
    "        parse_dialogue_until_2007(content, conference, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_dialogue_until_2007(content, conference, year):\n",
    "    splits = content.split('\\n\\n\\n')\n",
    "    data = {}\n",
    "    authors = []\n",
    "    for author in splits[1].split('\\n\\n'):\n",
    "        d = {}\n",
    "        d[NAME] = author.split('\\n\\n')[0]\n",
    "        d[EMAIL] = author.split('\\n\\n')[2]\n",
    "        d[AFFILIATION] = author.split('\\n\\n')[1]\n",
    "        authors.append(d)\n",
    "    data[AUTHOR] = authors\n",
    "    d = {}\n",
    "    d[TITLE] = splits[0]\n",
    "    d[ABSTRACT] = splits[2]\n",
    "    d[TEXT] = splits[3:]\n",
    "    data[TEXT] = d\n",
    "    data[CONFERENCE] = conference\n",
    "    data[YEAR] = year\n",
    "    result.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_dialogue_2007_plus(content, conference, year):\n",
    "    splits = content.split('\\n\\n\\n')\n",
    "    data = {}\n",
    "    authors = []\n",
    "    for author in splits[1].split('\\n\\n'):\n",
    "        d = {}\n",
    "        d[NAME] = author.split('(')[0]\n",
    "        d[EMAIL] = author[author.find('(') + 1:author.find(')')]\n",
    "        d[AFFILIATION] = author.split(')')[1]\n",
    "        authors.append(d)\n",
    "    data[AUTHOR] = authors\n",
    "    d = {}\n",
    "    d[TITLE] = splits[0]\n",
    "    d[ABSTRACT] = splits[2]\n",
    "    d[TEXT] = splits[3:]\n",
    "    data[TEXT] = d\n",
    "    data[CONFERENCE] = conference\n",
    "    data[YEAR] = year\n",
    "    result.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for root, dirs, files in walk(path.join('..', saving_dir, conferences[0]), 'r'):\n",
    "     for file in files:\n",
    "        with open(path.join(root, file), 'r') as input_stream:\n",
    "            if encounter_utility_file(file):\n",
    "                continue\n",
    "            year = root.split('/')[~0]\n",
    "            try:\n",
    "                add_paper_data(input_stream.read(), conferences[0], year)\n",
    "            except (UnicodeDecodeError, IndexError):\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('{}.pickle'.format(conferences[2]), 'wb') as f:\n",
    "    dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПАРСИМ АИСТ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_aist(content, conference, year):\n",
    "    splits = content.split('\\n\\n\\n')\n",
    "    data = {}\n",
    "    authors = []\n",
    "    for author in splits[1].split('\\n\\n'):\n",
    "        d = {}\n",
    "        d[NAME] = author.split('\\n')[0]\n",
    "        d[EMAIL] = author.split('\\n')[2]\n",
    "        d[AFFILIATION] = author.split('\\n')[1]\n",
    "        authors.append(d)\n",
    "    data[AUTHOR] = authors\n",
    "    d = {}\n",
    "    d[TITLE] = splits[0]\n",
    "    d[ABSTRACT] = splits[2]\n",
    "    d[KEYWORDS] = splits[3]\n",
    "    d[TEXT] = splits[4:]\n",
    "    data[TEXT] = d\n",
    "    data[CONFERENCE] = conference\n",
    "    data[YEAR] = year\n",
    "    result.append(data)\n",
    "    \n",
    "result = []\n",
    "\n",
    "for root, dirs, files in walk(path.join('..', saving_dir, conferences[1]), 'r'):\n",
    "     for file in files:\n",
    "        with open(path.join(root, file), 'r') as input_stream:\n",
    "            if encounter_utility_file(file):\n",
    "                continue\n",
    "            year = str(file.split('.')[0])\n",
    "            try:\n",
    "                papers = input_stream.read().split('==')\n",
    "                for paper in papers:\n",
    "                    parse_aist(paper, conferences[1], year)\n",
    "            except (UnicodeDecodeError, IndexError):\n",
    "                continue\n",
    "\n",
    "with open('{}.pickle'.format(conferences[1]), 'wb') as f:\n",
    "    dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПАРСИМ AINL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_ainl(content, conference, year):\n",
    "    splits = content.split('\\n\\n\\n')\n",
    "    data = {}\n",
    "    authors = []\n",
    "    for author in splits[1].split('\\n\\n'):\n",
    "        d = {}\n",
    "        d[NAME] = author.split('\\n')[0]\n",
    "        d[EMAIL] = author.split('\\n')[2]\n",
    "        d[AFFILIATION] = author.split('\\n')[1]\n",
    "        authors.append(d)\n",
    "    data[AUTHOR] = authors\n",
    "    d = {}\n",
    "    d[TITLE] = splits[0]\n",
    "    d[ABSTRACT] = splits[2]\n",
    "    # d[KEYWORDS] = splits[3]\n",
    "    d[TEXT] = splits[3:]\n",
    "    data[TEXT] = d\n",
    "    data[CONFERENCE] = conference\n",
    "    data[YEAR] = year\n",
    "    result.append(data)\n",
    "    \n",
    "result = []\n",
    "\n",
    "for root, dirs, files in walk(path.join('..', saving_dir, conferences[0]), 'r'):\n",
    "     for file in files:\n",
    "        with open(path.join(root, file), 'r') as input_stream:\n",
    "            if encounter_utility_file(file):\n",
    "                continue\n",
    "            year = root.split('/')[~0]\n",
    "            try:\n",
    "                papers = input_stream.read().split('==')\n",
    "                for paper in papers:\n",
    "                    parse_ainl(paper, conferences[0], year)\n",
    "            except (UnicodeDecodeError, IndexError):\n",
    "                continue\n",
    "\n",
    "with open('{}.pickle'.format(conferences[0]), 'wb') as f:\n",
    "     dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПАРСИМ RUSSIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_russir(content, conference, year):\n",
    "    splits = content.split('\\n\\n\\n')\n",
    "    data = {}\n",
    "    authors = []\n",
    "    for author in splits[1].split('\\n\\n'):\n",
    "        d = {}\n",
    "        d[NAME] = author.split('\\n')[0]\n",
    "        d[EMAIL] = author.split('\\n')[2]\n",
    "        d[AFFILIATION] = author.split('\\n')[1]\n",
    "        authors.append(d)\n",
    "    data[AUTHOR] = authors\n",
    "    d = {}\n",
    "    d[TITLE] = splits[0]\n",
    "    d[ABSTRACT] = splits[2]\n",
    "    # d[KEYWORDS] = splits[3]\n",
    "    d[TEXT] = splits[3:]\n",
    "    data[TEXT] = d\n",
    "    data[CONFERENCE] = conference\n",
    "    data[YEAR] = year\n",
    "    result.append(data)\n",
    "    \n",
    "result = []\n",
    "\n",
    "for root, dirs, files in walk(path.join('..', saving_dir, conferences[3]), 'r'):\n",
    "     for file in files:\n",
    "        with open(path.join(root, file), 'r') as input_stream:\n",
    "            if encounter_utility_file(file):\n",
    "                continue\n",
    "            year = root.split('/')[~0]\n",
    "            try:\n",
    "                papers = input_stream.read().split('==')\n",
    "                for paper in papers:\n",
    "                    parse_russir(paper, conferences[3], year)\n",
    "            except (UnicodeDecodeError, IndexError):\n",
    "                continue\n",
    "\n",
    "with open('{}.pickle'.format(conferences[3]), 'wb') as f:\n",
    "     dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Вот так вот!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
