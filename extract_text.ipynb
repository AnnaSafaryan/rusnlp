{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path, walk\n",
    "from collections import defaultdict\n",
    "from re import compile, sub\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_pdf_to_txt(path):\n",
    "    # print(path)\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = ''\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсим Диалог-2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dialogue_2004 = []\n",
    "\n",
    "for root, dirs, files in walk(path.join('..', 'data', 'AINL', '2017'), 'rb'):\n",
    "        for file in files:\n",
    "            if (str(file)) != '.DS_Store':\n",
    "                try:\n",
    "                    dialogue_2004.append(convert_pdf_to_txt('{}/{}'.format(root, file)))  \n",
    "                except:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data.pickle', 'wb') as f:\n",
    "    dump(dialogue_2004, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data.pickle', 'rb') as f:\n",
    "    texts = load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for tid, text in enumerate(texts):\n",
    "    with open('../prepared-data/AINL/2017/{}.txt'.format(tid), 'w') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсим остальное:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_to_nums = compile(\"([a-zA-Z]+)([0-9]+)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_bad(string):\n",
    "    return sub('\\t|\\xa0', '', string.replace('\\n', ' ')).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encounter_utility_file(filename):\n",
    "    if any(name in filename for name in ['DS_Store', 'zip']):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Парсим Диалог после 2010:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TITLE = 'title'\n",
    "AUTHOR = 'author'\n",
    "NAME = 'name'\n",
    "AFFILIATION = 'affiliation'\n",
    "EMAIL = 'email'\n",
    "TEXT = 'text'\n",
    "REFERENCES = 'references'\n",
    "REF_MARKER = '\\xa0 ЛИТЕРАТУРА \\xa0'\n",
    "KEYWORDS_MARKER = 'Ключевые слова'\n",
    "KEYWORDS = 'keywords'\n",
    "ABSTRACT = 'abstract'\n",
    "CONFERENCE = 'conference'\n",
    "YEAR = 'year'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПАРСИМ ДИАЛОГ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "for root, dirs, files in walk(path.join('..', 'prepared-data/Dialogue'), 'r'):\n",
    "     for file in files:\n",
    "        with open(path.join(root, file), 'r') as input_stream:\n",
    "            if encounter_utility_file(file):\n",
    "                continue\n",
    "            conference = 'Dialogue'\n",
    "            year = str(root).split('prepared-data/')[1].split('/')[1]\n",
    "            try:\n",
    "                add_paper_data(input_stream.read(), conference, year)\n",
    "            except (UnicodeDecodeError, IndexError):\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_paper_data(content, conference, year):\n",
    "    if int(year) > 2007:\n",
    "        parse_dialogue_2007_plus(content, conference, year)\n",
    "    else:\n",
    "        parse_dialogue_until_2007(content, conference, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_dialogue_until_2007(content, conference, year):\n",
    "    splits = content.split('\\n\\n\\n')\n",
    "    data = {}\n",
    "    authors = []\n",
    "    for author in splits[1].split('\\n\\n'):\n",
    "        d = {}\n",
    "        d[NAME] = author.split('\\n\\n')[0]\n",
    "        d[EMAIL] = author.split('\\n\\n')[2]\n",
    "        d[AFFILIATION] = author.split('\\n\\n')[1]\n",
    "        authors.append(d)\n",
    "    data[AUTHOR] = authors\n",
    "    d = {}\n",
    "    d[TITLE] = splits[0]\n",
    "    d[ABSTRACT] = splits[2]\n",
    "    d[TEXT] = splits[3:]\n",
    "    data[TEXT] = d\n",
    "    data[CONFERENCE] = conference\n",
    "    data[YEAR] = year\n",
    "    result.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_dialogue_2007_plus(content, conference, year):\n",
    "    splits = content.split('\\n\\n\\n')\n",
    "    data = {}\n",
    "    authors = []\n",
    "    for author in splits[1].split('\\n\\n'):\n",
    "        d = {}\n",
    "        d[NAME] = author.split('(')[0]\n",
    "        d[EMAIL] = author[author.find('(') + 1:author.find(')')]\n",
    "        d[AFFILIATION] = author.split(')')[1]\n",
    "        authors.append(d)\n",
    "    data[AUTHOR] = authors\n",
    "    d = {}\n",
    "    d[TITLE] = splits[0]\n",
    "    d[ABSTRACT] = splits[2]\n",
    "    d[TEXT] = splits[3:]\n",
    "    data[TEXT] = d\n",
    "    data[CONFERENCE] = conference\n",
    "    data[YEAR] = year\n",
    "    result.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('dialogue.pickle', 'wb') as f:\n",
    "    dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПАРСИМ АИСТ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_aist(content, conference, year):\n",
    "    splits = content.split('\\n\\n\\n')\n",
    "    data = {}\n",
    "    authors = []\n",
    "    for author in splits[1].split('\\n\\n'):\n",
    "        d = {}\n",
    "        d[NAME] = author.split('\\n')[0]\n",
    "        d[EMAIL] = author.split('\\n')[2]\n",
    "        d[AFFILIATION] = author.split('\\n')[1]\n",
    "        authors.append(d)\n",
    "    data[AUTHOR] = authors\n",
    "    d = {}\n",
    "    d[TITLE] = splits[0]\n",
    "    d[ABSTRACT] = splits[2]\n",
    "    d[KEYWORDS] = splits[3]\n",
    "    d[TEXT] = splits[4:]\n",
    "    data[TEXT] = d\n",
    "    data[CONFERENCE] = conference\n",
    "    data[YEAR] = year\n",
    "    result.append(data)\n",
    "    \n",
    "result = []\n",
    "\n",
    "for root, dirs, files in walk(path.join('..', 'prepared-data/AIST'), 'r'):\n",
    "     for file in files:\n",
    "        with open(path.join(root, file), 'r') as input_stream:\n",
    "            if encounter_utility_file(file):\n",
    "                continue\n",
    "            conference = 'AIST'\n",
    "            year = str(file.split('.')[0])\n",
    "            try:\n",
    "                papers = input_stream.read().split('==')\n",
    "                for paper in papers:\n",
    "                    parse_aist(paper, conference, year)\n",
    "            except (UnicodeDecodeError, IndexError):\n",
    "                continue\n",
    "\n",
    "with open('aist.pickle', 'wb') as f:\n",
    "    dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПАРСИМ AINL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_ainl(content, conference, year):\n",
    "    splits = content.split('\\n\\n\\n')\n",
    "    data = {}\n",
    "    authors = []\n",
    "    for author in splits[1].split('\\n\\n'):\n",
    "        d = {}\n",
    "        d[NAME] = author.split('\\n')[0]\n",
    "        d[EMAIL] = author.split('\\n')[2]\n",
    "        d[AFFILIATION] = author.split('\\n')[1]\n",
    "        authors.append(d)\n",
    "    data[AUTHOR] = authors\n",
    "    d = {}\n",
    "    d[TITLE] = splits[0]\n",
    "    d[ABSTRACT] = splits[2]\n",
    "    # d[KEYWORDS] = splits[3]\n",
    "    d[TEXT] = splits[3:]\n",
    "    data[TEXT] = d\n",
    "    data[CONFERENCE] = conference\n",
    "    data[YEAR] = year\n",
    "    result.append(data)\n",
    "    \n",
    "result = []\n",
    "\n",
    "for root, dirs, files in walk(path.join('..', 'prepared-data/AINL'), 'r'):\n",
    "     for file in files:\n",
    "        with open(path.join(root, file), 'r') as input_stream:\n",
    "            if encounter_utility_file(file):\n",
    "                continue\n",
    "            conference = 'AINL'\n",
    "            year = str(root).split('prepared-data/')[1].split('/')[1]\n",
    "            try:\n",
    "                papers = input_stream.read().split('==')\n",
    "                for paper in papers:\n",
    "                    parse_ainl(paper, conference, year)\n",
    "            except (UnicodeDecodeError, IndexError):\n",
    "                continue\n",
    "\n",
    "with open('ainl.pickle', 'wb') as f:\n",
    "    dump(result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "СТАРЫЙ ПАРСИНГ КОД (НЕВАЛИДЕН)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_files(root):\n",
    "    for root, dirs, files in walk(root, 'r'):\n",
    "        for file in files:\n",
    "            with open(path.join(root, file), 'rb') as input_stream:\n",
    "                if encounter_utility_file(file):\n",
    "                    continue\n",
    "                conference = str(root + file).split('data/')[1].split('/')[0]\n",
    "                year = str(root).split('data/')[1].split('/')[1]\n",
    "                try:\n",
    "                    add_paper_data(input_stream.read().decode('utf-8').split('\\n'), conference, year)\n",
    "                except UnicodeDecodeError:\n",
    "                    continue # remove\n",
    "                    reader = PdfFileReader(input_stream)\n",
    "                    data[conference][year].append(reader.getPage(0).extractText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "\n",
    "with open('dialogue_2010-2017_chunk.pickle', 'wb') as f:\n",
    "    dump(result[:10], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
